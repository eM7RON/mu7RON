{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# mu7RON Demo 2\n",
    "---\n",
    "\n",
    "In this tutorial we will prepare some midi sequences as input and train an LSTM or GRU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import copy\n",
    "import hashlib\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import midi\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mu7ron import analyze, aug, edit, generate, coders, learn, maps, utils, visualize, temporal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directories that contain my midi files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), r'..\\..\\..\\..\\__Data__\\Midi\\Bach')\n",
    "sub_dirs = [\n",
    "            \"Dave's\",\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 111.12it/s]\n"
     ]
    }
   ],
   "source": [
    "dirs = [os.path.join(data_dir, dir_) for dir_ in sub_dirs]\n",
    "\n",
    "#retrieve all filenames ending '.mid'\n",
    "fnames = []\n",
    "for dir_ in tqdm(dirs):\n",
    "    for root, subs, files in os.walk(dir_):\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith(('.mid', '.midi')):\n",
    "                fnames.append(os.path.join(root, fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up the midi files with python-midi. Some of my files have errors so I use a try/except block to catch exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 261/261 [00:13<00:00, 18.85it/s]\n"
     ]
    }
   ],
   "source": [
    "data   = []\n",
    "\n",
    "for fn in tqdm(fnames):\n",
    "    try:\n",
    "        data.append(midi.read_midifile(fn))\n",
    "    except (TypeError, AssertionError):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decide on some variables which we will need to keep constant through training and when we make predictions / generate music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_mode     = True   # whether to encode NoteOffEvents or just NoteOnEvents with 0 velocity\n",
    "q            = 18     # quantization factor of velocity\n",
    "q_map        = maps.create_q_map(128, q, encode=True, decode=True)\n",
    "t            = 8     # the smallest timestep in milliseconds\n",
    "n_vel        = utils.dynamic_order(128, q) # No. of different available levels of velocity\n",
    "n_time       = 48   # Available timesteps\n",
    "n_pitch      = 128 * 2 if off_mode else 128 # + Available pitches\n",
    "n_pulse      = 0     # Number of added pulses\n",
    "n_vocab      = n_time + n_pitch + n_vel + n_pulse # Available choices\n",
    "n_sample     = 240    # batch size\n",
    "n_input      = 120   # time steps - the temporal dimension\n",
    "n_output     = 1     # number of timesteps to predict into the future\n",
    "n_teach      = 7\n",
    "n_step       = n_input + n_output + n_sample - 2\n",
    "n_example    = 1\n",
    "buffer       = 150\n",
    "random_state = 117\n",
    "\n",
    "time_encoder = temporal.base_digits_encoder #time.timeslips_encoder\n",
    "time_decoder = temporal.base_digits_decoder #time.timeslips_decoder\n",
    "ekwa         = dict(b=n_time) #dict(t=t, n_time=n_time)\n",
    "dkwa         = dict(b=n_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will only include midi files that contain ONLY these instruments or NO\n",
    "# defined instruments (which defaults to acoustic piano)\n",
    "\n",
    "insts_to_keep = {\n",
    "    *range(40),\n",
    "#     45,\n",
    "#     *range(56, 105),\n",
    "#     *range(113, 128),\n",
    "}\n",
    "\n",
    "data = [x for x in data if analyze.has_only_inst(x, insts_to_keep)]\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [x for x in data if 1 < analyze.max_simulataneous_notes(x) < 4]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [00:06<00:00,  3.96it/s]\n"
     ]
    }
   ],
   "source": [
    "typs_2_keep = (\n",
    "             midi.NoteOffEvent,\n",
    "             midi.NoteOnEvent,\n",
    "             midi.SetTempoEvent,\n",
    "             midi.TimeSignatureEvent,\n",
    ")\n",
    "\n",
    "def copy_func(old_evnt, new_typ):\n",
    "    new_evnt = new_typ(\n",
    "                       tick=old_evnt.tick,\n",
    "                       data=[old_evnt.data[0], 0],\n",
    "                       channel=old_evnt.channel,\n",
    "                       )\n",
    "    return new_evnt\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    \n",
    "    # remove redundant events types\n",
    "    data[i] = edit.filter_ptrn_of_evnt_typs(data[i], typs_2_keep)\n",
    "    \n",
    "    # filter out NotOnEvents on channel 9 (10); which is used exclusively for percussion\n",
    "    data[i] = edit.filter_ptrn_of_percussion(data[i])\n",
    "    \n",
    "    # combine all Track in the Pattern into a single Track\n",
    "    data[i] = edit.consolidate_trcks(data[i])\n",
    "    \n",
    "    # adjust resolution to 480 but also adjust all tempo evnts as to preserve the sounding\n",
    "    # speed of the music\n",
    "    data[i] = edit.normalize_resolution(data[i], res=480)\n",
    "    # finally save\n",
    "    \n",
    "    # we will quantize velocity into a fewer possible values\n",
    "    data[i] = edit.quantize_typ_attr(data[i], q, (midi.NoteOnEvent, midi.NoteOffEvent), lambda x: x.data[1])\n",
    "    \n",
    "    # consolidating trcks leads to a lot of redundent/duplicate evnts which we can remove\n",
    "    data[i] = edit.dedupe(data[i])\n",
    "    \n",
    "    if not off_mode:\n",
    "        data[i] = edit.replace_evnt_typ(data[i], midi.NoteOffEvent, midi.NoteOnEvent, copy_func=copy_func)\n",
    "    \n",
    "    # split each ptrn where a timesignature event occurs\n",
    "    data[i] = edit.split_on_timesignature_change(data[i], midi.TimeSignatureEvent)\n",
    "    \n",
    "data = list(utils.flatten(data, depth=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = edit.filter_data_of_empty_ptrn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 37/37 [00:01<00:00, 24.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# We need to remove timesignature events\n",
    "\n",
    "typs_2_keep=(\n",
    "             midi.NoteOffEvent,\n",
    "             midi.NoteOnEvent,\n",
    "             midi.SetTempoEvent,\n",
    ")\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    # remove redundant events types\n",
    "    data[i] = edit.filter_ptrn_of_evnt_typs(data[i], typs_2_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ptrn[0] for ptrn in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 37/37 [00:00<00:00, 107.25it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ = [coders.categorize_input(x, \n",
    "                                 q=q,\n",
    "                                 q_map=q_map,\n",
    "                                 time_encoder=time_encoder,\n",
    "                                 ekwa=ekwa,\n",
    "                                 n_time=n_time,\n",
    "                                 n_vel=n_vel,\n",
    "                                 sort_velocity=True,\n",
    "                                 sort_pitch=True,\n",
    "                                 sort_pulse=False,\n",
    "                                 asarray=True, \n",
    "                                 dtype='int',\n",
    "                                 off_mode=off_mode,\n",
    "                                 ) for x in tqdm(data)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_state)\n",
    "\n",
    "idx = np.random.permutation(range(len(input_)))\n",
    "input_ = np.array(input_)[idx]\n",
    "test_frac = 0.2\n",
    "n         = len(input_)\n",
    "idx       = int(round(n * (1. - test_frac)))\n",
    "train, valid = input_[: idx],  input_[idx: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 samples were removed because they are too short. There are 22 remaining. \n",
      "If this is not enough, try reducing n_step, n_teach and/or buffer and create a new instance of MappedDataAugGen\n"
     ]
    }
   ],
   "source": [
    "train_gen = aug.MappedDataAugGen(train,\n",
    "                     teacher_forcing=True,\n",
    "                     n_teach=n_teach, #n_teach,\n",
    "                     n_step=n_step,\n",
    "                     buffer=buffer,\n",
    "                     time_encoder=time_encoder,\n",
    "                     time_decoder=time_decoder,\n",
    "                     ekwa=ekwa,\n",
    "                     dkwa=dkwa,\n",
    "                     n_time=n_time,\n",
    "                     n_vocab=n_vocab,\n",
    "                     n_sample=n_sample, \n",
    "                     n_input=n_input, \n",
    "                     n_output=n_output,\n",
    "                     off_mode=off_mode,\n",
    "                     lo_lim=60,\n",
    "                     hi_lim=90,\n",
    "                     time_aug_range=[0.95, 1., 1.05],\n",
    "                     )\n",
    "valid_gen = aug.MappedBalancedDataGen(valid, n_input=n_input, n_example=n_example, n_output=n_output, n_sample=n_sample, n_vocab=n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU, Activation\n",
    "from tensorflow.compat.v1 import ConfigProto, Session\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy, CategoricalCrossentropy\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = Session(config=config)\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Nadam(\n",
    "                      learning_rate=0.001,\n",
    "                      beta_1=0.9,\n",
    "                      beta_2=0.999,\n",
    "                      epsilon=1e-07,\n",
    "                      name=\"Nadam\",\n",
    "                      clipnorm=True,\n",
    "                      )\n",
    "\n",
    "\n",
    "class CustomCheckpoint(Callback):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_dir  = kwargs.get('save_dir', os.path.join(os.getcwd(), 'models'))\n",
    "        self.monitor   = kwargs.get('monitor', 'val_loss')\n",
    "        self.verbose   = kwargs.get('verbose', True)\n",
    "        self.save_mode = kwargs.get('save_mode', 'save') # save_weights\n",
    "        \n",
    "        if self.monitor.endswith('loss'):\n",
    "            self.best  = np.inf\n",
    "            self.oper  = operator.lt\n",
    "        else:\n",
    "            self.best  = -np.inf\n",
    "            self.oper  = operator.gt\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        imp = False\n",
    "        if self.oper(logs[self.monitor], self.best):\n",
    "            imp = True\n",
    "            val = str(logs[self.monitor]).replace('.', '_')\n",
    "            save_path = os.path.join(self.save_dir, f\"{utils.tstamp(f'model_{val}')}.h5\")\n",
    "\n",
    "        if self.verbose:\n",
    "            if imp:\n",
    "                msg = f\"{self.monitor} improved from {self.best} to {logs[self.monitor]}, saving model to {save_path}\" \n",
    "            else:\n",
    "                msg = f\"val_loss did not improve from {self.best}\"\n",
    "            print(f\"epoch: {epoch} {msg} loss: {round(logs['loss'], 4)} acc: {round(logs['categorical_accuracy'], 4)} val_acc: {round(logs['val_categorical_accuracy'], 4)} val_loss: {round(logs['val_loss'], 4)}\")\n",
    "            \n",
    "        if imp:\n",
    "            getattr(self.model, self.save_mode)(save_path, overwrite=True)\n",
    "            self.best = logs[self.monitor]\n",
    "\n",
    "\n",
    "autosaver = CustomCheckpoint()\n",
    "\n",
    "log_dir = os.path.join(os.getcwd(), 'logs', utils.tstamp('mugen_lstm'))\n",
    "tensorboard = TensorBoard(log_dir=log_dir, profile_batch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 120, 240)          398880    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 120, 240)          0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 240)               347040    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 240)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 312)               75192     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 312)               0         \n",
      "=================================================================\n",
      "Total params: 821,112\n",
      "Trainable params: 821,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(240, \n",
    "               input_shape=(n_input, n_vocab),\n",
    "               return_sequences=True,\n",
    "               ))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(GRU(240))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(\n",
    "              loss='categorical_crossentropy', \n",
    "              optimizer=opt,\n",
    "              metrics=['categorical_accuracy'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 2835 steps, validate for 2 steps\n",
      "Epoch 1/1000\n",
      "2834/2835 [============================>.] - ETA: 0s - loss: 2.4972 - categorical_accuracy: 0.3927epoch: 0 val_loss improved from inf to 3.940481662750244, saving model to W:\\OneDrive\\__Dev__\\__Dev__\\AI Music\\MusicEvo Project\\models\\model_3_940481662750244_19_Sep_2020_18-49-13.h5 loss: 2.4973 acc: 0.3926999866962433 val_acc: 0.08749999850988388 val_loss: 3.9405\n",
      "2835/2835 [==============================] - 303s 107ms/step - loss: 2.4973 - categorical_accuracy: 0.3927 - val_loss: 3.9405 - val_categorical_accuracy: 0.0875\n",
      "Epoch 2/1000\n",
      " 659/2835 [=====>........................] - ETA: 3:51 - loss: 1.8922 - categorical_accuracy: 0.5076"
     ]
    }
   ],
   "source": [
    "model.fit(train_gen, epochs=1000, verbose=True, callbacks=[tensorboard, autosaver], validation_data=valid_gen, workers=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MusicGen",
   "language": "python",
   "name": "musicgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
